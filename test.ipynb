{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/mila/l/le.zhang/scratch/colxlip/src/\")\n",
    "import torch\n",
    "from colxlip.factory import create_model_and_transforms\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess_train, preprocess_val = create_model_and_transforms(model_name=\"ViT-B-16-colxlip\", pretrained=\"/home/mila/l/le.zhang/scratch/colxlip/src/logs/2025_04_08-01_14_39-model_ViT-B-16-colxlip-lr_5e-06-b_196-j_8-p_amp/checkpoints/epoch_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "winoground = load_dataset(\"facebook/winoground\")\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_colbert_similarity(token_image_features, token_text_features):\n",
    "    \"\"\"\n",
    "    Compute token-level similarity. Given relative information between image and text tokens,\n",
    "    we only compute similarity from text tokens to image tokens, without considering the reverse.\n",
    "    This is based on the assumption that the image tokens are more informative than the text tokens, \n",
    "    and we assume each text token is associated with image tokens while not vice versa.\n",
    "    \n",
    "    Args:\n",
    "        token_image_features: Token-level features from images [batch_size_img, n_img_tokens, embed_dim]\n",
    "        token_text_features: Token-level features from text [batch_size_txt, n_txt_tokens, embed_dim]\n",
    "        \n",
    "    Returns:\n",
    "        Token-level similarity matrix [batch_size_txt, batch_size_img], similar to global similarity, each entry with value in [-1, 1]\n",
    "    \"\"\"\n",
    "    sim_matrix = torch.einsum('mnd,kqd->mknq', token_text_features, token_image_features)\n",
    "    max_sim_per_txt_token = torch.max(sim_matrix, dim=3)[0]  # [batch_size_txt, batch_size_img, n_txt_tokens]\n",
    "    \n",
    "    # Create a mask for non-zero values\n",
    "    mask = (max_sim_per_txt_token != 0).float()\n",
    "    # Sum of non-zero values\n",
    "    sum_sim = torch.sum(max_sim_per_txt_token, dim=2)\n",
    "    # Count of non-zero values (adding small epsilon to avoid division by zero)\n",
    "    count = torch.sum(mask, dim=2) + 1e-8\n",
    "    # Average of non-zero values\n",
    "    logits_per_text_token = sum_sim / count  # [batch_size_txt, batch_size_img]\n",
    "  \n",
    "    return logits_per_text_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "winoground_test = winoground['test']\n",
    "example_idx = 155\n",
    "\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "ax1.title.set_text('image_0')\n",
    "plt.imshow(winoground_test[example_idx][\"image_0\"].convert(\"RGB\"))\n",
    "\n",
    "ax2 = plt.subplot(1, 3, 2)\n",
    "ax2.title.set_text('image_1')\n",
    "plt.imshow(winoground_test[example_idx][\"image_1\"].convert(\"RGB\"))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"caption_0:\", winoground_test[example_idx][\"caption_0\"])\n",
    "print(\"caption_1:\", winoground_test[example_idx][\"caption_1\"])\n",
    "\n",
    "# Define the preprocess function from your model\n",
    "preprocess = preprocess_val\n",
    "\n",
    "# Note that some images in winoground are RGBA and some are RGB. Need to convert all to RGB with .convert('RGB')\n",
    "# Process images and text\n",
    "image1 = preprocess(winoground_test[example_idx][\"image_0\"].convert(\"RGB\")).unsqueeze(0)\n",
    "image2 = preprocess(winoground_test[example_idx][\"image_1\"].convert(\"RGB\")).unsqueeze(0)\n",
    "images = torch.cat([image1, image2], dim=0)\n",
    "\n",
    "text = tokenizer([winoground_test[example_idx][\"caption_0\"], winoground_test[example_idx][\"caption_1\"]])\n",
    "\n",
    "# Get model outputs\n",
    "with torch.no_grad():\n",
    "    image_features, token_image_features = model.encode_image(images)\n",
    "    text_features, token_text_features = model.encode_text(text)\n",
    "    \n",
    "    # Normalize features\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    token_image_features = token_image_features / token_image_features.norm(dim=-1, keepdim=True)\n",
    "    token_text_features = token_text_features / token_text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Calculate similarity scores\n",
    "    logits_per_image = image_features @ text_features.T\n",
    "    logits_per_text_tokens = compute_colbert_similarity(token_image_features, token_text_features)\n",
    "    logits_per_image_tokens = logits_per_text_tokens.T\n",
    "    \n",
    "    # Print scores\n",
    "    print(\"\\nCLIP image-text match scores:\")\n",
    "    print(f\"image_0, caption_0: {logits_per_image[0][0].item():.4f}\")\n",
    "    print(f\"image_0, caption_1: {logits_per_image[0][1].item():.4f}\")\n",
    "    print(f\"image_1, caption_0: {logits_per_image[1][0].item():.4f}\")\n",
    "    print(f\"image_1, caption_1: {logits_per_image[1][1].item():.4f}\")\n",
    "\n",
    "    print(\"\\nColBERT image-text match scores:\")\n",
    "    print(f\"image_0, caption_0: {logits_per_image_tokens[0][0].item():.4f}\")\n",
    "    print(f\"image_0, caption_1: {logits_per_image_tokens[0][1].item():.4f}\")\n",
    "    print(f\"image_1, caption_0: {logits_per_image_tokens[1][0].item():.4f}\")\n",
    "    print(f\"image_1, caption_1: {logits_per_image_tokens[1][1].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:21<00:00,  2.83it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "winoground_clip_scores = []\n",
    "winoground_colbert_scores = []\n",
    "winoground_combined_scores = []\n",
    "\n",
    "for example in tqdm(winoground['test']):\n",
    "    # Process images and text\n",
    "    image1 = preprocess(example[\"image_0\"].convert(\"RGB\")).unsqueeze(0)\n",
    "    image2 = preprocess(example[\"image_1\"].convert(\"RGB\")).unsqueeze(0)\n",
    "    images = torch.cat([image1, image2], dim=0)\n",
    "    \n",
    "    text = tokenizer([example[\"caption_0\"], example[\"caption_1\"]])\n",
    "    \n",
    "    # Get model outputs\n",
    "    with torch.no_grad():\n",
    "        image_features, token_image_features = model.encode_image(images)\n",
    "        text_features, token_text_features = model.encode_text(text)\n",
    "        \n",
    "        # Normalize features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        token_image_features = token_image_features / token_image_features.norm(dim=-1, keepdim=True)\n",
    "        token_text_features = token_text_features / token_text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Calculate similarity scores\n",
    "        logits_per_image = image_features @ text_features.T\n",
    "        logits_per_text_tokens = compute_colbert_similarity(token_image_features, token_text_features)\n",
    "        logits_per_image_tokens = logits_per_text_tokens.T\n",
    "        \n",
    "        # Extract the four scores for CLIP\n",
    "        clip_score_c0_i0 = logits_per_image[0][0].item()\n",
    "        clip_score_c1_i0 = logits_per_image[0][1].item()\n",
    "        clip_score_c0_i1 = logits_per_image[1][0].item()\n",
    "        clip_score_c1_i1 = logits_per_image[1][1].item()\n",
    "        \n",
    "        # Extract the four scores for ColBERT\n",
    "        colbert_score_c0_i0 = logits_per_image_tokens[0][0].item()\n",
    "        colbert_score_c1_i0 = logits_per_image_tokens[0][1].item()\n",
    "        colbert_score_c0_i1 = logits_per_image_tokens[1][0].item()\n",
    "        colbert_score_c1_i1 = logits_per_image_tokens[1][1].item()\n",
    "        \n",
    "        combined_score_c0_i0 = 0.5 * clip_score_c0_i0 + 0.5 * colbert_score_c0_i0\n",
    "        combined_score_c1_i0 = 0.5 * clip_score_c1_i0 + 0.5 * colbert_score_c1_i0\n",
    "        combined_score_c0_i1 = 0.5 * clip_score_c0_i1 + 0.5 * colbert_score_c0_i1\n",
    "        combined_score_c1_i1 = 0.5 * clip_score_c1_i1 + 0.5 * colbert_score_c1_i1\n",
    "    \n",
    "    # Store CLIP scores\n",
    "    winoground_clip_scores.append({\n",
    "        \"id\": example[\"id\"], \n",
    "        \"c0_i0\": clip_score_c0_i0, \n",
    "        \"c0_i1\": clip_score_c0_i1, \n",
    "        \"c1_i0\": clip_score_c1_i0, \n",
    "        \"c1_i1\": clip_score_c1_i1\n",
    "    })\n",
    "    \n",
    "    # Store ColBERT scores\n",
    "    winoground_colbert_scores.append({\n",
    "        \"id\": example[\"id\"], \n",
    "        \"c0_i0\": colbert_score_c0_i0, \n",
    "        \"c0_i1\": colbert_score_c0_i1, \n",
    "        \"c1_i0\": colbert_score_c1_i0, \n",
    "        \"c1_i1\": colbert_score_c1_i1\n",
    "    })\n",
    "    \n",
    "    # Store combined scores\n",
    "    winoground_combined_scores.append({\n",
    "        \"id\": example[\"id\"], \n",
    "        \"c0_i0\": combined_score_c0_i0, \n",
    "        \"c0_i1\": combined_score_c0_i1, \n",
    "        \"c1_i0\": combined_score_c1_i0, \n",
    "        \"c1_i1\": combined_score_c1_i1\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照0.3:0.7的权重计算combined score\n",
    "winoground_combined_scores = []\n",
    "for i,j in zip(winoground_clip_scores, winoground_colbert_scores):\n",
    "    winoground_combined_scores.append({\n",
    "        \"id\": i[\"id\"], \n",
    "        \"c0_i0\": 0.7 * i[\"c0_i0\"] + 0.3 * j[\"c0_i0\"], \n",
    "        \"c0_i1\": 0.7 * i[\"c0_i1\"] + 0.3 * j[\"c0_i1\"], \n",
    "        \"c1_i0\": 0.7 * i[\"c1_i0\"] + 0.3 * j[\"c1_i0\"], \n",
    "        \"c1_i1\": 0.7 * i[\"c1_i1\"] + 0.3 * j[\"c1_i1\"]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text score: 0.2775\n",
      "image score: 0.0925\n",
      "group score: 0.07\n",
      "colbert text score: 0.1975\n",
      "colbert image score: 0.125\n",
      "colbert group score: 0.07\n",
      "combined text score: 0.28\n",
      "combined image score: 0.0925\n",
      "combined group score: 0.06\n"
     ]
    }
   ],
   "source": [
    "def text_correct(result):\n",
    "    return result[\"c0_i0\"] > result[\"c1_i0\"] and result[\"c1_i1\"] > result[\"c0_i1\"]\n",
    "\n",
    "def image_correct(result):\n",
    "    return result[\"c0_i0\"] > result[\"c0_i1\"] and result[\"c1_i1\"] > result[\"c1_i0\"]\n",
    "\n",
    "def group_correct(result):\n",
    "    return image_correct(result) and text_correct(result)\n",
    "\n",
    "text_correct_count = 0\n",
    "image_correct_count = 0\n",
    "group_correct_count = 0\n",
    "\n",
    "colbert_text_correct_count = 0\n",
    "colbert_image_correct_count = 0\n",
    "colbert_group_correct_count = 0\n",
    "\n",
    "combined_text_correct_count = 0\n",
    "combined_image_correct_count = 0\n",
    "combined_group_correct_count = 0\n",
    "\n",
    "for result in winoground_clip_scores:\n",
    "  text_correct_count += 1 if text_correct(result) else 0\n",
    "  image_correct_count += 1 if image_correct(result) else 0\n",
    "  group_correct_count += 1 if group_correct(result) else 0\n",
    "\n",
    "denominator = len(winoground_clip_scores)\n",
    "print(\"text score:\", text_correct_count/denominator)\n",
    "print(\"image score:\", image_correct_count/denominator)\n",
    "print(\"group score:\", group_correct_count/denominator)\n",
    "\n",
    "for result in winoground_colbert_scores:\n",
    "  colbert_text_correct_count += 1 if text_correct(result) else 0\n",
    "  colbert_image_correct_count += 1 if image_correct(result) else 0\n",
    "  colbert_group_correct_count += 1 if group_correct(result) else 0\n",
    "\n",
    "print(\"colbert text score:\", colbert_text_correct_count/denominator)\n",
    "print(\"colbert image score:\", colbert_image_correct_count/denominator)\n",
    "print(\"colbert group score:\", colbert_group_correct_count/denominator)\n",
    "\n",
    "for result in winoground_combined_scores:\n",
    "  combined_text_correct_count += 1 if text_correct(result) else 0\n",
    "  combined_image_correct_count += 1 if image_correct(result) else 0\n",
    "  combined_group_correct_count += 1 if group_correct(result) else 0\n",
    "\n",
    "print(\"combined text score:\", combined_text_correct_count/denominator)\n",
    "print(\"combined image score:\", combined_image_correct_count/denominator)\n",
    "print(\"combined group score:\", combined_group_correct_count/denominator)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16', pretrained='laion400m_e32')\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "winoground = load_dataset(\"facebook/winoground\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:19<00:00,  2.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "winoground_clip_scores = []\n",
    "winoground_colbert_scores = []\n",
    "winoground_combined_scores = []\n",
    "\n",
    "for example in tqdm(winoground['test']):\n",
    "    # Process images and text\n",
    "    image1 = preprocess(example[\"image_0\"].convert(\"RGB\")).unsqueeze(0)\n",
    "    image2 = preprocess(example[\"image_1\"].convert(\"RGB\")).unsqueeze(0)\n",
    "    images = torch.cat([image1, image2], dim=0)\n",
    "    \n",
    "    text = tokenizer([example[\"caption_0\"], example[\"caption_1\"]])\n",
    "    \n",
    "    # Get model outputs\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(images)\n",
    "        text_features = model.encode_text(text)\n",
    "        \n",
    "        # Normalize features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logits_per_image = image_features @ text_features.T\n",
    "        # Extract the four scores for CLIP\n",
    "        clip_score_c0_i0 = logits_per_image[0][0].item()\n",
    "        clip_score_c1_i0 = logits_per_image[0][1].item()\n",
    "        clip_score_c0_i1 = logits_per_image[1][0].item()\n",
    "        clip_score_c1_i1 = logits_per_image[1][1].item()\n",
    "        \n",
    "   \n",
    "    # Store CLIP scores\n",
    "    winoground_clip_scores.append({\n",
    "        \"id\": example[\"id\"], \n",
    "        \"c0_i0\": clip_score_c0_i0, \n",
    "        \"c0_i1\": clip_score_c0_i1, \n",
    "        \"c1_i0\": clip_score_c1_i0, \n",
    "        \"c1_i1\": clip_score_c1_i1\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_correct(result):\n",
    "    return result[\"c0_i0\"] > result[\"c1_i0\"] and result[\"c1_i1\"] > result[\"c0_i1\"]\n",
    "\n",
    "def image_correct(result):\n",
    "    return result[\"c0_i0\"] > result[\"c0_i1\"] and result[\"c1_i1\"] > result[\"c1_i0\"]\n",
    "\n",
    "def group_correct(result):\n",
    "    return image_correct(result) and text_correct(result)\n",
    "\n",
    "text_correct_count = 0\n",
    "image_correct_count = 0\n",
    "group_correct_count = 0\n",
    "\n",
    "for result in winoground_clip_scores:\n",
    "  text_correct_count += 1 if text_correct(result) else 0\n",
    "  image_correct_count += 1 if image_correct(result) else 0\n",
    "  group_correct_count += 1 if group_correct(result) else 0\n",
    "\n",
    "denominator = len(winoground_clip_scores)\n",
    "print(\"text score:\", text_correct_count/denominator)\n",
    "print(\"image score:\", image_correct_count/denominator)\n",
    "print(\"group score:\", group_correct_count/denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
